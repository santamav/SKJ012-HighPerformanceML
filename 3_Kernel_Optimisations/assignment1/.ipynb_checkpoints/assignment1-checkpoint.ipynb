{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This downloads the CIFAR-10 dataset to your Drive\n",
    "# if it doesn't already exist.\n",
    "%cd sjk012/datasets/\n",
    "!bash get_datasets.sh\n",
    "%cd ../../\n",
    "\n",
    "!pip install numpy matplotlib threadpoolctl imageio cython\n",
    "\n",
    "# Download and install the OpenBLAS and BLIS libraries\n",
    "%mkdir -p sjk012/software\n",
    "%cd sjk012/software\n",
    "%rm -rf blis\n",
    "!git clone https://github.com/flame/blis.git \n",
    "%cd blis/\n",
    "!./configure --prefix=${HOME}/install/blis --enable-cblas --enable-threading=openmp auto &> /dev/null\n",
    "!make -j4 install \n",
    "%cd ..\n",
    "%rm -rf OpenBLAS\n",
    "!git clone https://github.com/OpenMathLib/OpenBLAS.git\n",
    "%cd OpenBLAS\n",
    "!make -j4 &> /dev/null\n",
    "!make PREFIX=${HOME}/install/openblas install\n",
    "%cd ../../../\n",
    "%env LD_LIBRARY_PATH=$LD_LIBRARY_PATH:${HOME}/install/blis/lib:${HOME}/install/openblas/lib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TbwXz499i6qQ",
    "tags": [
     "pdf-ignore"
    ]
   },
   "source": [
    "# Fully-Connected Neural Nets\n",
    "In this first assignment we will implement and parallelise fully-connected networks using a modular approach. For each layer we will implement a `forward` and a `backward` function. The `forward` function will receive inputs, weights, and other parameters and will return both an output and a `cache` object storing data needed for the backward pass, like this:\n",
    "\n",
    "```python\n",
    "def layer_forward(x, w):\n",
    "  \"\"\" Receive inputs x and weights w \"\"\"\n",
    "  # Do some computations ...\n",
    "  z = # ... some intermediate value\n",
    "  # Do some more computations ...\n",
    "  out = # the output\n",
    "   \n",
    "  cache = (x, w, z, out) # Values we need to compute gradients\n",
    "   \n",
    "  return out, cache\n",
    "```\n",
    "\n",
    "The backward pass will receive upstream derivatives and the `cache` object, and will return gradients with respect to the inputs and weights, like this:\n",
    "\n",
    "```python\n",
    "def layer_backward(dout, cache):\n",
    "  \"\"\"\n",
    "  Receive dout (derivative of loss with respect to outputs) and cache,\n",
    "  and compute derivative with respect to inputs.\n",
    "  \"\"\"\n",
    "  # Unpack cache values\n",
    "  x, w, z, out = cache\n",
    "  \n",
    "  # Use values in cache to compute derivatives\n",
    "  dx = # Derivative of loss with respect to x\n",
    "  dw = # Derivative of loss with respect to w\n",
    "  \n",
    "  return dx, dw\n",
    "```\n",
    "\n",
    "After implementing a bunch of layers this way, we will be able to easily combine them to build classifiers with different architectures.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As usual, a bit of setup\n",
    "from __future__ import print_function\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sjk012.data_utils import get_CIFAR10_data\n",
    "from sjk012.gradient_check import eval_numerical_gradient, eval_numerical_gradient_array\n",
    "from sjk012.solver import Solver\n",
    "from sjk012.operations import *\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "  \"\"\" returns relative error \"\"\"\n",
    "  return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the (preprocessed) CIFAR10 data.\n",
    "data = get_CIFAR10_data()\n",
    "\n",
    "for k, v in list(data.items()):\n",
    "  print(('%s: ' % k, v.shape))\n",
    "\n",
    "data['X_train']= data['X_train'].reshape(data['X_train'].shape[0], np.prod(data['X_train'].shape[1:])).astype(np.float32)\n",
    "data['X_val']  = data['X_val'  ].reshape(data['X_val'  ].shape[0], np.prod(data['X_val'  ].shape[1:])).astype(np.float32)\n",
    "data['X_test'] = data['X_test' ].reshape(data['X_test' ].shape[0], np.prod(data['X_test' ].shape[1:])).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive matmul operator:\n",
    "Open the file `sjk012/operations.py` and implement the `matmul_naive` function.\n",
    "\n",
    "Once you are done you can test your implementation by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_shape = (3, 5)\n",
    "b_shape = (5, 4)\n",
    "c_shape = (3, 4)\n",
    "\n",
    "a = np.linspace(-0.1, 0.5, num=np.prod(a_shape), dtype=np.float32).reshape(a_shape)\n",
    "b = np.linspace(-0.2, 0.3, num=np.prod(b_shape), dtype=np.float32).reshape(b_shape)\n",
    "c = np.linspace(-0.3, 0.1, num=np.prod(c_shape), dtype=np.float32).reshape(c_shape)\n",
    "\n",
    "c_out_naive = matmul_naive(a, b, c)\n",
    "\n",
    "correct_c = np.array([[-0.2556391,  -0.22115516, -0.18667122, -0.15218729],\n",
    "                      [-0.09890636, -0.03622693,  0.02645249,  0.08913192],\n",
    "                      [ 0.05782638,  0.1487013,   0.23957621,  0.33045113]])\n",
    "\n",
    "# Compare your output with ours. The error should be around e-7 or less.\n",
    "print('Testing matmul_naive function:')\n",
    "print('difference: ', rel_error(c_out_naive, correct_c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy matmul operator:\n",
    "Open the file `sjk012/operations.py` and implement the `matmul_numpy` function.\n",
    "\n",
    "Once you are done you can test your implementation running the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "c = np.linspace(-0.3, 0.1, num=np.prod(c_shape), dtype=np.float32).reshape(c_shape)\n",
    "\n",
    "c_out_np = matmul_numpy(a, b, c)\n",
    "\n",
    "# Compare your output with ours. The error should be around e-7 or less.\n",
    "print('Testing matmul_numpy function:')\n",
    "print('difference: ', rel_error(c_out_np, correct_c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cblas_gemm:\n",
    "Open the file `sjk012/operations.py` and implement the `matmul_cblas` function in order to call to `cblas_sgemm` BLAS3 function. See: https://netlib.org/lapack/explore-html/de/da0/cblas_8h_a1446cddceb275e7cd299157a5d61d5e4.html\n",
    "\n",
    "Here's a simple example demonstrating how to call an external function using ctypes:\n",
    "\n",
    "```python\n",
    "import ctypes\n",
    "\n",
    "# Load the library containing the external function\n",
    "libc = ctypes.CDLL('/path/to/your/library.so')  # Replace '/path/to/your/library.so' with the path to your library\n",
    "\n",
    "# Define the argument types and return type of the external function\n",
    "libc.my_external_function.argtypes = [ctypes.c_int, ctypes.c_int]\n",
    "libc.my_external_function.restype = ctypes.c_int\n",
    "\n",
    "# Call the external function\n",
    "result = libc.my_external_function(3, 4)\n",
    "```\n",
    "\n",
    "Once you are done, add the location of the OpenBLAS and BLIS libraries to the `LD_LIBRARY_PATH` environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['LD_LIBRARY_PATH'] = os.environ['LD_LIBRARY_PATH'] + \":\" + os.environ['HOME'] + \"/install/blis/lib:\" + os.environ['HOME'] + \"/install/openblas/lib\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afterwards, you can test your implementation using OpenBLAS running the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "c = np.linspace(-0.3, 0.1, num=np.prod(c_shape), dtype=np.float32).reshape(c_shape)\n",
    "openblas = load_library(\"openblas\")\n",
    "\n",
    "c_out_cblas = matmul_cblas(openblas, a, b, c)\n",
    "\n",
    "# Compare your output with ours. The error should be around e-7 or less.\n",
    "print('Testing matmul_cblas function:')\n",
    "print('difference: ', rel_error(c_out_cblas, correct_c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try the same as before but using the BLIS library: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = np.linspace(-0.3, 0.1, num=np.prod(c_shape), dtype=np.float32).reshape(c_shape)\n",
    "blis = load_library(\"blis\")\n",
    "\n",
    "c_out_cblas = matmul_cblas(blis, a, b, c)\n",
    "\n",
    "# Compare your output with ours. The error should be around e-7 or less.\n",
    "print('Testing matmul_cblas function:')\n",
    "print('difference: ', rel_error(c_out_cblas, correct_c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tiled GEMM multiplication:\n",
    "\n",
    "In this task, you will implement tiling for the inner loops of the GEMM operation. The idea is to divide the matrices into smaller tiles and then perform the multiplication on these tiles. The following pseudo-code and figure illustrate the concept of tiling for the inner loops of GEMM. You need to also parallelize the outer loop using OpenMP.\n",
    "\n",
    "![Tiled GEMM](sjk012/img/tiled_gemm.png) \n",
    "\n",
    "Open the file `sjk012/tiled_gemm/tiled_gemm.pyx` and implement the `matmul_tiled_cython_inner`.\n",
    "\n",
    "```\n",
    "function matmul_tiled_cython_inner(A, B, C, M, N, K, blockSize):\n",
    "    for m from 0 to M step blockSize:\n",
    "        for n from 0 to N step blockSize:\n",
    "            for k from 0 to K step blockSize:\n",
    "\n",
    "                // Calculate upper bounds for each block\n",
    "                mUpper = min(m + blockSize, M)\n",
    "                nUpper = min(n + blockSize, N)\n",
    "                kUpper = min(k + blockSize, K)\n",
    "\n",
    "                for i from m to mUpper:\n",
    "                    for j from n to nUpper:\n",
    "                        temp = 0\n",
    "                        for l from k to kUpper:\n",
    "                            temp += A[i][l] * B[l][j]\n",
    "                        C[i][j] += temp\n",
    "end function\n",
    "```\n",
    "\n",
    "Use the `prange` function from Cython to parallelize the first loop via OpenMP. See https://cython.readthedocs.io/en/latest/src/userguide/parallelism.html\n",
    "\n",
    "Open the file `sjk012/operations.py` and implement the `matmul_tiled` function in order to call to `tiled_gemm` function that you have just implemented. \n",
    "\n",
    "Afterwards, you can test your implementation running the code below. Note that the code will automatically compile the Cython module and will store it in `~/.pyxbld`. If you want to compile it manually, run `python\n",
    "setup.py build_ext --inplace`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "c = np.linspace(-0.3, 0.1, num=np.prod(c_shape), dtype=np.float32).reshape(c_shape)\n",
    "\n",
    "import pyximport\n",
    "pyximport.install(reload_support=True, pyimport=True)\n",
    "from sjk012.tiled_gemm.tiled_gemm import matmul_tiled_cython\n",
    "\n",
    "c_out_tiled = matmul_tiled_cython(a, b, c, 32)\n",
    "# Compare your output with ours. The error should be around e-7 or less.\n",
    "print('Testing matmul_tiled function:')\n",
    "print('difference: ', rel_error(c_out_tiled, correct_c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tiled gemm block size performance evaluation:\n",
    "\n",
    "Now that we have implemented the tiled GEMM, we have to evaluatethe optimal block size for the target machine. Try using differnet number of threads to see what could be the best option. \n",
    "\n",
    "*You can write here your conclusions:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sjk012.operations import *\n",
    "from sjk012.fc_net import *\n",
    "from threadpoolctl import threadpool_limits\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Create numpy arrays for matrices a, b, and c\n",
    "m, n, k = 2048, 2048, 2048\n",
    "\n",
    "a = np.random.rand(m, k).astype(np.float32)\n",
    "b = np.random.rand(k, n).astype(np.float32)\n",
    "\n",
    "with threadpool_limits(limits=8):\n",
    "    tile_sizes = [4, 8, 16, 32, 64, 128, 256, 512]\n",
    "\n",
    "    ###########################################################################\n",
    "    # TODO: Call the matmul_tiled_cython passing above defined tile_sizes     #\n",
    "    # and obtain the execution time for each size                             #\n",
    "    ###########################################################################\n",
    "    \n",
    "    # ...\n",
    "        \n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel performance evaluation\n",
    "\n",
    "Now, evaluate the performance of the different versions using the corresponding parallel versions.\n",
    "\n",
    "First, set the corresponding location of `libgomp.so` to the `LD_LIBRARY_PATH` environment variable (you would probably need to adjust the path where `libgomp.so` is located in your platform):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['LD_LIBRARY_PATH'] = os.environ['LD_LIBRARY_PATH'] + \":\" + \"/usr/lib/gcc/x86_64-linux-gnu/9/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the code running the matmul implementations in parallel using OpenMP threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threadpoolctl import threadpool_limits\n",
    "from pprint import pprint\n",
    "\n",
    "# Define matrix sizes\n",
    "sizes = [2**i for i in range(4, 12)]\n",
    "\n",
    "# Number of repetitions\n",
    "repetitions = 1\n",
    "tile_size = 16\n",
    "\n",
    "thread_range = [1, 2, 4, 8, 16, 24]\n",
    "\n",
    "# Initialize arrays to store execution times\n",
    "numpy_times = np.zeros((len(sizes), len(thread_range)))\n",
    "openblas_times = np.zeros((len(sizes), len(thread_range)))\n",
    "blis_times = np.zeros((len(sizes), len(thread_range)))\n",
    "tiled_times = np.zeros((len(sizes), len(thread_range)))\n",
    "\n",
    "for num_threads, n_threads in enumerate(thread_range):\n",
    "    with threadpool_limits(limits=n_threads):\n",
    "        \n",
    "        # Measure performance for each size\n",
    "        for idx, size in enumerate(sizes):\n",
    "            print(f\"Matrix size: {size} x {size}, Threads: {n_threads}\")\n",
    "\n",
    "            # Initialize arrays to store execution times for current size\n",
    "            numpy_times_size = []\n",
    "            openblas_times_size = []\n",
    "            blis_times_size = []\n",
    "            tiled_times_size = []\n",
    "\n",
    "            for _ in range(repetitions):\n",
    "                ###########################################################################\n",
    "                # TODO: Call the corresponding versions of the matmul and append the times #\n",
    "                # to the previously created vectors                                       #\n",
    "                ###########################################################################\n",
    "                # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****                  \n",
    "                # Generate random matrices\n",
    "                # ...\n",
    "                # Measure performance of numpy implementation\n",
    "                # ...\n",
    "                 \n",
    "                # Measure performance of tiled implementation\n",
    "                # ...\n",
    "\n",
    "                # Measure performance of cblas implementation with OpenBLAS\n",
    "                # ...\n",
    "\n",
    "                # Measure performance of cblas implementation with BLIS\n",
    "                # ...\n",
    "\n",
    "                # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "                ###########################################################################\n",
    "                #                             END OF YOUR CODE                            #\n",
    "                ###########################################################################            \n",
    "\n",
    "            # Compute average execution times for current size and number of threads\n",
    "            numpy_times[idx, num_threads] = np.mean(numpy_times_size)\n",
    "            openblas_times[idx, num_threads] = np.mean(openblas_times_size)\n",
    "            blis_times[idx, num_threads] = np.mean(blis_times_size)\n",
    "            tiled_times[idx, num_threads] = np.mean(tiled_times_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the execution times and observe the times of each matmul version running with different number of threads. \n",
    "\n",
    "Which version scales better with regards to the number of threads and matrix size? What is the most efficient?\n",
    "\n",
    "*Write here your observations:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting subplots\n",
    "fig, axs = plt.subplots(2, 3, figsize=(12, 8), sharex=True, sharey=True)\n",
    "\n",
    "for i, n_threads in enumerate([1, 2, 4, 8, 16, 24]):\n",
    "    row = i // 3\n",
    "    col = i % 3\n",
    "    axs[row, col].plot(sizes, numpy_times[:, i], marker='o', label=f'NumPy')\n",
    "    axs[row, col].plot(sizes, tiled_times[:, i], marker='.', label=f'Tiled')\n",
    "    axs[row, col].plot(sizes, openblas_times[:, i], marker='x', label=f'OpenBLAS')\n",
    "    axs[row, col].plot(sizes, blis_times[:, i], marker='+', label=f'BLIS')\n",
    "    axs[row, col].set_title(f'Threads: {n_threads}')\n",
    "    axs[row, col].set_xlabel('Matrix Size')\n",
    "    axs[row, col].set_ylabel('Execution Time (seconds)')\n",
    "    axs[row, col].legend()\n",
    "    axs[row, col].set_yscale(\"log\")\n",
    "    axs[row, col].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot now the speedups of the different matmul versions and write your observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting subplots\n",
    "fig, axs = plt.subplots(2, 3, figsize=(12, 8), sharex=True, sharey=True)\n",
    "\n",
    "for i, n_threads in enumerate([1, 2, 4, 8, 16, 24]):\n",
    "    row = i // 3\n",
    "    col = i % 3\n",
    "    axs[row, col].plot(sizes, numpy_times[:, 0] / numpy_times[:, i], marker='o', label=f'NumPy')\n",
    "    axs[row, col].plot(sizes, tiled_times[:, 0] / tiled_times[:, i], marker='.', label=f'Tiled')\n",
    "    axs[row, col].plot(sizes, openblas_times[:, 0] / openblas_times[:, i], marker='x', label=f'OpenBLAS')\n",
    "    axs[row, col].plot(sizes, blis_times[:, 0] / blis_times[:, i], marker='+', label=f'BLIS')\n",
    "    axs[row, col].set_title(f'Threads: {n_threads}')\n",
    "    axs[row, col].set_xlabel('Matrix Size')\n",
    "    axs[row, col].set_ylabel('Speedup')\n",
    "    axs[row, col].legend()\n",
    "    axs[row, col].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully-Connected Layer: Forward Pass\n",
    "\n",
    "To implement the forward pass for the fully-connected layer, open the file `sjk012/layers.py` and implement the `fc_forward` function. This function utilizes matrix multiplication (`matmul`) to perform the forward pass efficiently.\n",
    "\n",
    "### Forward Pass Formula:\n",
    "\n",
    "Given input data \\($X$\\), weights \\($W$\\), and biases \\($b$\\), the forward pass for the fully-connected layer can be defined as:\n",
    "\n",
    "$$\\text{out} = X \\times W + b$$\n",
    "\n",
    "### Implementation:\n",
    "\n",
    "1. **Matrix Multiplication**: Perform matrix multiplication between input data \\($X$\\) and weights \\($W$\\).\n",
    "2. **Add Bias**: Add the biases \\($b$\\) to the result of the matrix multiplication.\n",
    "\n",
    "Once you complete the implementation of the `fc_forward` function, you can test its correctness by running the provided code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the fc_forward function\n",
    "from sjk012.layers import fc_forward\n",
    "\n",
    "num_inputs = 2\n",
    "input_shape = (4, 5, 6)\n",
    "output_dim = 3\n",
    "\n",
    "input_size = num_inputs * np.prod(input_shape)\n",
    "weight_size = output_dim * np.prod(input_shape)\n",
    "x = np.linspace(-0.1, 0.5, num=input_size, dtype=np.float32).reshape(num_inputs, np.prod(input_shape))\n",
    "w = np.linspace(-0.2, 0.3, num=weight_size, dtype=np.float32).reshape(np.prod(input_shape), output_dim)\n",
    "b = np.linspace(-0.3, 0.1, num=output_dim, dtype=np.float32)\n",
    "\n",
    "out, _ = fc_forward(x, w, b)\n",
    "\n",
    "correct_out = np.array([[ 1.49834967,  1.70660132,  1.91485297],\n",
    "                        [ 3.25553199,  3.5141327,   3.77273342]])\n",
    "\n",
    "# Compare your output with ours. The error should be around e-9 or less.\n",
    "print('Testing fc_forward function:')\n",
    "print('difference: ', rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully-Connected Layer: Backward Pass\n",
    "\n",
    "To implement the backward pass for the fully-connected layer, you need to open the file `sjk012/layers.py` and implement the `fc_backward` function.\n",
    "\n",
    "### Backward Pass Formula:\n",
    "\n",
    "The backward pass for the fully-connected layer involves computing gradients with respect to the input data \\($X$\\), weights \\($W$\\), and biases \\($b$\\). \n",
    "\n",
    "The gradients with respect to \\($X$\\), \\($W$\\), and \\($b$\\) can be computed using the chain rule and are given by:\n",
    "\n",
    "$$\\nabla X = \\nabla Y \\times W^T$$\n",
    "\n",
    "$$\\nabla W = X^T \\times \\nabla Y$$\n",
    "\n",
    "$$\\nabla b = \\text{sum}(\\nabla Y, axis=0)$$\n",
    "\n",
    "Where \\($L$\\) is the loss function and \\($\\nabla Y$\\) is the gradient of the loss with respect to the output of the fully-connected layer.\n",
    "\n",
    "### Implementation:\n",
    "\n",
    "1. **Gradient with Respect to Input**: Perform matrix multiplication between upstream gradient \\($\\nabla Y$\\) and the transposed weights weights \\($W^T$\\).\n",
    "2. **Gradient with Respect to Weights**: Perform matrix multiplication between transposed inputs \\($X^T$\\) and the upstream gradient \\($\\nabla Y$\\).\n",
    "3. **Gradient with Respect to Biases**: Compute the gradient with respect to biases \\($\\nabla b$\\) using the formula above.\n",
    "\n",
    "Once you complete the implementation of the `fc_backward` function, you can test its correctness using numeric gradient checking.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 302,
     "status": "ok",
     "timestamp": 1628606505973,
     "user": {
      "displayName": "MANTAS BIRŠKUS",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh4p74VzcLbLKZFYUnmVzxKmHhFZC9ouHcczEsEmQ=s64",
      "userId": "00995227095641424292"
     },
     "user_tz": -180
    },
    "id": "lp5uUk5Wi6qc",
    "outputId": "afbe43cb-17e1-43c7-de13-56f1e2d2f09c"
   },
   "outputs": [],
   "source": [
    "# Test the fc_backward function\n",
    "from sjk012.layers import fc_backward\n",
    "\n",
    "np.random.seed(12)\n",
    "x = np.random.randn(10, 2, 3).astype(np.float32).reshape(10, 2*3)\n",
    "w = np.random.randn(6, 5).astype(np.float32)\n",
    "b = np.random.randn(5).astype(np.float32)\n",
    "dy = np.random.randn(10, 5).astype(np.float32)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: fc_forward(x, w, b)[0], x, dy, h=1e-1)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: fc_forward(x, w, b)[0], w, dy, h=1e-1)\n",
    "db_num = eval_numerical_gradient_array(lambda b: fc_forward(x, w, b)[0], b, dy, h=1e-1)\n",
    "\n",
    "_, cache = fc_forward(x, w, b)\n",
    "dx, dw, db = fc_backward(dy, cache)\n",
    "\n",
    "# The error should be around e-5 or less\n",
    "print('Testing fc_backward function:')\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print('dw error: ', rel_error(dw_num, dw))\n",
    "print('db error: ', rel_error(db_num, db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Fh5rO5ni6qf"
   },
   "source": [
    "# ReLU Activation: Forward Pass\n",
    "\n",
    "To implement the forward pass for the ReLU activation function, you need to open the file `sjk012/layers.py` and implement the `relu_forward_numpy` function.\n",
    "\n",
    "### Forward Pass Formula:\n",
    "\n",
    "The forward pass for the ReLU activation function applies an element-wise operation to the input data \\($X$\\) and is given by:\n",
    "\n",
    "$$Y = \\text{max}(0, X)$$\n",
    "$$\\text{mask} = X > 0$$\n",
    "\n",
    "### Implementation:\n",
    "\n",
    "1. **Element-wise Operation**: Apply the ReLU function element-wise to the input data \\($X$\\), replacing negative values with zeros.\n",
    "2. **Mask Calculation**: Calculate the mask $ \\text{mask} = X > 0 $, which will be used for the backward pass to compute gradients.\n",
    "\n",
    "Once you complete the implementation of the `relu_forward_numpy` function, you can test its correctness using the provided code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 271,
     "status": "ok",
     "timestamp": 1628606508241,
     "user": {
      "displayName": "MANTAS BIRŠKUS",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh4p74VzcLbLKZFYUnmVzxKmHhFZC9ouHcczEsEmQ=s64",
      "userId": "00995227095641424292"
     },
     "user_tz": -180
    },
    "id": "30JzWSqsi6qh",
    "outputId": "cff8bd7b-653a-4dfc-c634-4818a674db22"
   },
   "outputs": [],
   "source": [
    "# Test the relu_forward function\n",
    "from sjk012.layers import relu_forward_numpy\n",
    "\n",
    "x = np.linspace(-0.5, 0.5, num=12, dtype=np.float32).reshape(3, 4)\n",
    "\n",
    "out, _ = relu_forward_numpy(x)\n",
    "correct_out = np.array([[ 0.,          0.,          0.,          0.,        ],\n",
    "                        [ 0.,          0.,          0.04545455,  0.13636364,],\n",
    "                        [ 0.22727273,  0.31818182,  0.40909091,  0.5,       ]])\n",
    "\n",
    "# Compare your output with ours. The error should be on the order of e-8\n",
    "print('Testing relu_forward function:')\n",
    "print('difference: ', rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU Activation: Forward Pass with Cython:\n",
    "\n",
    "1. **Element-wise Operation**: Apply the ReLU function element-wise to the input data \\( X \\), replacing negative values with zeros.\n",
    "2. **Parallelization**: Utilize OpenMP parallelization to efficiently compute the ReLU operation across multiple threads.\n",
    "\n",
    "Once you complete the implementation of the forward pass in `relu_fwd.pyx`, you need to compile the Cython module containing this function. You can compile it by executing the provided code or by running `python setup.py build_ext --inplace` in the previous directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "# Test the relu_cython function\n",
    "from sjk012.layers import relu_forward_cython\n",
    "\n",
    "import pyximport\n",
    "pyximport.install(reload_support=True, pyimport=True)\n",
    "from sjk012.relu_fwd.relu_fwd import relu_fwd_cython\n",
    "\n",
    "x = np.linspace(-0.5, 0.5, num=12, dtype=np.float32).reshape(3, 4)\n",
    "\n",
    "out, _ = relu_forward_cython(x)\n",
    "correct_out = np.array([[ 0.,          0.,          0.,          0.,        ],\n",
    "                        [ 0.,          0.,          0.04545455,  0.13636364,],\n",
    "                        [ 0.22727273,  0.31818182,  0.40909091,  0.5,       ]])\n",
    "\n",
    "\n",
    "# Compare your output with ours. The error should be on the order of e-8\n",
    "print('Testing relu_forward function:')\n",
    "print('difference: ', rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReLU Parallel Performance Evaluation\n",
    "\n",
    "To evaluate the performance of different versions of ReLU activation function using parallelization, follow these steps:\n",
    "\n",
    "1. **Set Library Location**: Add the location of `libgomp.so` to the `LD_LIBRARY_PATH` environment variable. This ensures that the OpenMP library is accessible during execution.\n",
    "\n",
    "2. **Performance Evaluation**: Compare the performance of different versions of ReLU activation function with and without parallelization. Measure execution times for each version and observe the impact of parallelization on speedup.\n",
    "\n",
    "Below is a sample code snippet demonstrating how to set the library location and perform performance evaluation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['LD_LIBRARY_PATH'] = os.environ['LD_LIBRARY_PATH'] + \":\" + \"/usr/lib/gcc/x86_64-linux-gnu/9/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threadpoolctl import threadpool_limits\n",
    "from pprint import pprint\n",
    "\n",
    "# Define matrix sizes\n",
    "sizes = [2**i for i in range(4, 14)]\n",
    "\n",
    "# Number of repetitions\n",
    "repetitions = 10\n",
    "\n",
    "thread_range = [1, 2, 4, 8, 16, 24]\n",
    "\n",
    "# Initialize arrays to store execution times\n",
    "relu_numpy_times = np.zeros((len(sizes), len(thread_range)))\n",
    "relu_cython_times = np.zeros((len(sizes), len(thread_range)))\n",
    "\n",
    "for num_threads, n_threads in enumerate(thread_range):\n",
    "    with threadpool_limits(limits=n_threads):\n",
    "        \n",
    "        # Measure performance for each size\n",
    "        for idx, size in enumerate(sizes):\n",
    "            print(f\"Matrix size: {size} x {size}, Threads: {n_threads}\")\n",
    "\n",
    "            # Initialize arrays to store execution times for current size\n",
    "            relu_numpy_times_size = []\n",
    "            relu_cython_times_size = []\n",
    "\n",
    "            for _ in range(repetitions):\n",
    "                ###########################################################################\n",
    "                # TODO: Call the corresponding versions of the relu and append the times  #\n",
    "                # to the previously created vectors                                       #\n",
    "                ###########################################################################\n",
    "                # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****                  \n",
    "                # Generate random matrices\n",
    "                # ...\n",
    "                \n",
    "                # Measure performance of numpy implementation\n",
    "                # ...\n",
    "\n",
    "                # Measure performance of tiled implementation\n",
    "                # ...\n",
    "\n",
    "                # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "                ###########################################################################\n",
    "                #                             END OF YOUR CODE                            #\n",
    "                ###########################################################################            \n",
    "\n",
    "            # Compute average execution times for current size and number of threads\n",
    "            relu_numpy_times[idx, num_threads] = np.mean(relu_numpy_times_size)\n",
    "            relu_cython_times[idx, num_threads] = np.mean(relu_cython_times_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting subplots\n",
    "fig, axs = plt.subplots(2, 3, figsize=(12, 8), sharex=True, sharey=True)\n",
    "\n",
    "for i, n_threads in enumerate([1, 2, 4, 8, 16, 24]):\n",
    "    row = i // 3\n",
    "    col = i % 3\n",
    "    axs[row, col].plot(sizes, relu_numpy_times[:, i], marker='o', label=f'ReLU Numpy')\n",
    "    axs[row, col].plot(sizes, relu_cython_times[:, i], marker='.', label=f'ReLU Cython')\n",
    "    axs[row, col].set_title(f'Threads: {n_threads}')\n",
    "    axs[row, col].set_xlabel('Matrix Size')\n",
    "    axs[row, col].set_ylabel('Execution Time (seconds)')\n",
    "    axs[row, col].legend()\n",
    "    axs[row, col].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D1fIjPTji6qi"
   },
   "source": [
    "# ReLU Activation: Backward Pass\n",
    "\n",
    "To implement the backward pass for the ReLU activation function, open the file `sjk012/layers.py` and implement the `relu_backward` function.\n",
    "\n",
    "### Backward Pass Formula:\n",
    "\n",
    "The backward pass for the ReLU activation function involves computing the gradient with respect to the input \\($X$\\). The gradient is given by:\n",
    "\n",
    "$$\\nabla X = \\nabla Y \\cdot \\text{mask}$$\n",
    "\n",
    "Where $ \\nabla Y $ is the upstream gradient and $ \\text{mask} $ is a binary mask indicating which elements of the input were positive during the forward pass.\n",
    "\n",
    "### Implementation:\n",
    "\n",
    "1. **Gradient with Respect to Input**: Compute the gradient with respect to the input data \\($\\nabla X$\\) using the formula above.\n",
    "\n",
    "Once you complete the implementation of the `relu_backward` function, you can test its correctness using numeric gradient checking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 239,
     "status": "ok",
     "timestamp": 1628606510316,
     "user": {
      "displayName": "MANTAS BIRŠKUS",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh4p74VzcLbLKZFYUnmVzxKmHhFZC9ouHcczEsEmQ=s64",
      "userId": "00995227095641424292"
     },
     "user_tz": -180
    },
    "id": "WV2dzeCui6qk",
    "outputId": "50b8423b-882e-4f0d-8e73-d11f0733140c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sjk012.layers import relu_backward_numpy\n",
    "\n",
    "np.random.seed(12)\n",
    "x = np.random.randn(10, 10).astype(np.float32)\n",
    "dy = np.random.randn(*x.shape).astype(np.float32)\n",
    "\n",
    "dy_num = eval_numerical_gradient_array(lambda x: relu_forward_numpy(x)[0], x, dy, h=1e-3)\n",
    "\n",
    "_, cache = relu_forward_numpy(x)\n",
    "dy = relu_backward_numpy(dy, cache)\n",
    "\n",
    "# The error should be on the order of e-5\n",
    "print('Testing relu_backward function:')\n",
    "print('dx error: ', rel_error(dy_num, dy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU Activation: Forward Pass with Cython:\n",
    "\n",
    "To implement the backward pass for the ReLU activation function using Cython, follow these steps:\n",
    "\n",
    "1. **Element-wise Operation**: Compute the gradient of the loss with respect to the input \\($\\nabla X$\\) by multiplying the gradient of the loss with respect to the output of the ReLU layer by a binary mask. This mask should indicate which elements of the input were positive during the forward pass.\n",
    "\n",
    "2. **Parallelization**: Utilize OpenMP parallelization to efficiently compute the backward pass operation across multiple threads.\n",
    "\n",
    "Once you complete the implementation of the backward pass in `relu_bwd.pyx`, you need to compile the Cython module containing this function. You can compile it by executing the provided code or by running `python setup.py build_ext --inplace` in the previous directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "# Test the relu_backward function\n",
    "from sjk012.layers import relu_backward_cython\n",
    "\n",
    "import pyximport\n",
    "pyximport.install(reload_support=True, pyimport=True)\n",
    "from sjk012.relu_bwd.relu_bwd import relu_bwd_cython\n",
    "\n",
    "np.random.seed(12)\n",
    "x = np.random.randn(10, 10).astype(np.float32)\n",
    "dy = np.random.randn(*x.shape).astype(np.float32)\n",
    "\n",
    "dy_num = eval_numerical_gradient_array(lambda x: relu_forward_cython(x)[0], x, dy, h=1e-3)\n",
    "\n",
    "_, cache = relu_forward_cython(x)\n",
    "dy = relu_backward_cython(dy, cache)\n",
    "\n",
    "# The error should be on the order of e-5\n",
    "print('Testing relu_forward function:')\n",
    "print('dx error: ', rel_error(dy_num, dy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XDhSDxP5i6qm"
   },
   "source": [
    "# \"Sandwich\" layers\n",
    "There are some common patterns of layers that are frequently used in neural nets. For example, fully-connected layers are frequently followed by a ReLU nonlinearity. To make these common patterns easy, we define several convenience layers in the file `sjk012/layer_utils.py`.\n",
    "\n",
    "For now take a look at the `fc_relu_forward` and `fc_relu_backward` functions, and run the following to numerically gradient check the backward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 269,
     "status": "ok",
     "timestamp": 1628606514524,
     "user": {
      "displayName": "MANTAS BIRŠKUS",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh4p74VzcLbLKZFYUnmVzxKmHhFZC9ouHcczEsEmQ=s64",
      "userId": "00995227095641424292"
     },
     "user_tz": -180
    },
    "id": "pAbzCJXMi6qo",
    "outputId": "e23c8ffe-91eb-46f9-b6ff-e182dab0f44f"
   },
   "outputs": [],
   "source": [
    "from sjk012.layer_utils import fc_relu_forward, fc_relu_backward\n",
    "\n",
    "np.random.seed(12)\n",
    "x = np.random.randn(2, 12).astype(np.float32)\n",
    "w = np.random.randn(12, 10).astype(np.float32)\n",
    "b = np.random.randn(10).astype(np.float32)\n",
    "dy = np.random.randn(2, 10).astype(np.float32)\n",
    "\n",
    "y, cache = fc_relu_forward(x, w, b)\n",
    "dx, dw, db = fc_relu_backward(dy, cache)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: fc_relu_forward(x, w, b)[0], x, dy, h=1e-1)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: fc_relu_forward(x, w, b)[0], w, dy, h=1e-1)\n",
    "db_num = eval_numerical_gradient_array(lambda b: fc_relu_forward(x, w, b)[0], b, dy, h=1e-2)\n",
    "\n",
    "# Relative error should be around e-3 or less\n",
    "print('Testing fc_relu_forward and fc_relu_backward:')\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print('dw error: ', rel_error(dw_num, dw))\n",
    "print('db error: ', rel_error(db_num, db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ztwlgprvi6qo"
   },
   "source": [
    "# Loss layers: Softmax\n",
    "Now you have to implement the loss and gradient for the softmax in the `softmax_loss` function in `sjk012/layers.py`.\n",
    "\n",
    "### Implementation\n",
    "Follow these steps to implement the provided code:\n",
    "\n",
    "1. **Compute the Softmax Probabilities**:\n",
    "\n",
    "   The softmax probabilities $ P $ for each class $ i $ are computed as follows:\n",
    "\n",
    "   $$ P_i = \\frac{e^{x_i - \\max(x)}}{\\sum_{j} e^{x_j - \\max(x)}} $$\n",
    "\n",
    "   Where:\n",
    "   - $ x $ is the input matrix of shape $(N, C)$ where $ N $ is the number of samples and $ C $ is the number of classes.\n",
    "   - $ \\max(x) $ computes the maximum value along each sample's row to improve numerical stability.\n",
    "\n",
    "2. **Compute the Softmax Loss**:\n",
    "\n",
    "   The softmax loss is calculated as the average cross-entropy loss over all samples:\n",
    "\n",
    "   $$ \\text{loss} = -\\frac{1}{N}\\sum_{i=1}^{N} \\log(P_{i, y_i}) $$\n",
    "\n",
    "   Where:\n",
    "   - $ N $ is the number of samples.\n",
    "   - $ y_i $ is the true label of sample $ i $.\n",
    "   - $ P_{i, y_i} $ denotes the predicted probability of the true class for sample $ i $.\n",
    "\n",
    "3. **Compute the Softmax Gradient**:\n",
    "\n",
    "   The gradient of the softmax loss with respect to the input scores $ x $ is given by:\n",
    "\n",
    "   $$ \\frac{\\partial L}{\\partial x_i} = \\frac{1}{N} (P_i - \\text{one\\_hot}(y_i)) $$\n",
    "\n",
    "   Where:\n",
    "   - $ P_i $ is the predicted probability of class $ i $.\n",
    "   - $ \\text{one\\_hot}(y_i) $ is a one-hot encoded vector representing the true class of sample $ i $.\n",
    "   - $ N $ is the number of samples, and dividing by $ N $ ensures averaging the gradients across all samples.\n",
    "    \n",
    "You can make sure that the implementations are correct by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 278,
     "status": "ok",
     "timestamp": 1628606517031,
     "user": {
      "displayName": "MANTAS BIRŠKUS",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh4p74VzcLbLKZFYUnmVzxKmHhFZC9ouHcczEsEmQ=s64",
      "userId": "00995227095641424292"
     },
     "user_tz": -180
    },
    "id": "IdqKNPSvi6qo",
    "outputId": "08105e2e-7400-4a1e-bba2-300318a57571"
   },
   "outputs": [],
   "source": [
    "from sjk012.layers import softmax_loss\n",
    "\n",
    "np.random.seed(12)\n",
    "num_classes, num_inputs = 10, 50\n",
    "x = 0.001 * np.random.randn(num_inputs, num_classes).astype(np.float32)\n",
    "y = np.random.randint(num_classes, size=num_inputs)\n",
    "\n",
    "dx_num = eval_numerical_gradient(lambda x: softmax_loss(x, y)[0], x, verbose=False, h=1e-1)\n",
    "loss, dx = softmax_loss(x, y)\n",
    "\n",
    "# Test softmax_loss function. Loss should be close to 2.3 and dx error should be around e-3\n",
    "print('\\nTesting softmax_loss:')\n",
    "print('loss: ', loss)\n",
    "print('dx error: ', rel_error(dx_num, dx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_tmI7VJri6qp"
   },
   "source": [
    "# Two-layer network\n",
    "Open the file `sjk012/fc_net.py` and complete the implementation of the `TwoLayerNet` class. Read through it to make sure you understand the API. You can run the cell below to test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1006,
     "status": "ok",
     "timestamp": 1628606520224,
     "user": {
      "displayName": "MANTAS BIRŠKUS",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh4p74VzcLbLKZFYUnmVzxKmHhFZC9ouHcczEsEmQ=s64",
      "userId": "00995227095641424292"
     },
     "user_tz": -180
    },
    "id": "rZG9wKUKi6qr",
    "outputId": "98f41330-6491-4959-8f5e-fffac8d6a33f"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sjk012.fc_net import TwoLayerNet\n",
    "\n",
    "np.random.seed(12)\n",
    "N, D, H, C = 3, 5, 50, 7\n",
    "X = np.random.randn(N, D).astype(np.float32)\n",
    "y = np.random.randint(C, size=N)\n",
    "\n",
    "std = 1e-3\n",
    "model = TwoLayerNet(input_dim=D, hidden_dim=H, num_classes=C, weight_scale=std)\n",
    "\n",
    "print('Testing initialization ... ')\n",
    "W1_std = abs(model.params['W1'].std() - std)\n",
    "b1 = model.params['b1']\n",
    "W2_std = abs(model.params['W2'].std() - std)\n",
    "b2 = model.params['b2']\n",
    "assert W1_std < std / 10, 'First layer weights do not seem right'\n",
    "assert np.all(b1 == 0), 'First layer biases do not seem right'\n",
    "assert W2_std < std / 10, 'Second layer weights do not seem right'\n",
    "assert np.all(b2 == 0), 'Second layer biases do not seem right'\n",
    "\n",
    "print('Testing test-time forward pass ... ')\n",
    "model.params['W1'] = np.linspace(-0.7, 0.3, num=D*H).reshape(D, H).astype(np.float32)\n",
    "model.params['b1'] = np.linspace(-0.1, 0.9, num=H).astype(np.float32)\n",
    "model.params['W2'] = np.linspace(-0.3, 0.4, num=H*C).reshape(H, C).astype(np.float32)\n",
    "model.params['b2'] = np.linspace(-0.9, 0.1, num=C).astype(np.float32)\n",
    "X = np.linspace(-5.5, 4.5, num=N*D).reshape(D, N).T.astype(np.float32)\n",
    "scores = model.loss(X)\n",
    "correct_scores = np.asarray(\n",
    "  [[11.53165108,  12.2917344,   13.05181771,  13.81190102,  14.57198434, 15.33206765,  16.09215096],\n",
    "   [12.05769098,  12.74614105,  13.43459113,  14.1230412,   14.81149128, 15.49994135,  16.18839143],\n",
    "   [12.58373087,  13.20054771,  13.81736455,  14.43418138,  15.05099822, 15.66781506,  16.2846319 ]])\n",
    "scores_diff = np.abs(scores - correct_scores).sum()\n",
    "print(scores_diff)\n",
    "assert scores_diff < 1e-4, 'Problem with test-time forward pass'\n",
    "\n",
    "print('Testing training loss (no regularization)')\n",
    "y = np.asarray([0, 5, 1])\n",
    "loss, grads = model.loss(X, y)\n",
    "correct_loss = 3.4702243556\n",
    "print(abs(loss - correct_loss))\n",
    "assert abs(loss - correct_loss) < 1e-6, 'Problem with training-time loss'\n",
    "\n",
    "model.reg = 1.0\n",
    "loss, grads = model.loss(X, y)\n",
    "correct_loss = 26.5948426952\n",
    "print(abs(loss - correct_loss))\n",
    "assert abs(loss - correct_loss) < 1e-6, 'Problem with regularization loss'\n",
    "\n",
    "# Errors should be around e-7 or less\n",
    "for reg in [0.0, 0.7]:\n",
    "  print('Running numeric gradient check with reg = ', reg)\n",
    "  model.reg = reg\n",
    "  loss, grads = model.loss(X, y)\n",
    "\n",
    "  for name in sorted(grads):\n",
    "    f = lambda _: model.loss(X, y)[0]\n",
    "    grad_num = eval_numerical_gradient(f, model.params[name], verbose=False, h=1e-2)\n",
    "    print('%s relative error: %.2e' % (name, rel_error(grad_num, grads[name])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5s5ltPgRi6q1"
   },
   "source": [
    "# Solver\n",
    "Open the file `sjk012/solver.py` and read through it to familiarize yourself with the API. You also need to implement the `sgd` function in `sjk012/optim.py`. The Stochastic Gradient Descent (SGD) formula is:\n",
    "\n",
    "$$W_{t+1} = W_t - \\eta \\cdot \\nabla W$$\n",
    "\n",
    "where:\n",
    "- $W_t$ are the weights/biases at time step $t$,\n",
    "- $\\eta$ is the learning rate,\n",
    "- $\\nabla W_t$ is the gradient of the loss function with respect to weights/biases.\n",
    "\n",
    "After implementing the SGD function, use a Solver instance to train a TwoLayerNet that achieves about 36% accuracy on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 43926,
     "status": "ok",
     "timestamp": 1628606569386,
     "user": {
      "displayName": "MANTAS BIRŠKUS",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh4p74VzcLbLKZFYUnmVzxKmHhFZC9ouHcczEsEmQ=s64",
      "userId": "00995227095641424292"
     },
     "user_tz": -180
    },
    "id": "uFKoZ3n0i6q1",
    "outputId": "f6698504-7b81-44b6-fd8e-4de9d95b59a8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from threadpoolctl import threadpool_limits\n",
    "\n",
    "input_size = 32 * 32 * 3\n",
    "hidden_size = 50\n",
    "num_classes = 10\n",
    "model = TwoLayerNet(input_size, hidden_size, num_classes)\n",
    "solver = None\n",
    "\n",
    "##############################################################################\n",
    "# TODO: Use a Solver instance to train a TwoLayerNet that achieves about 40% #\n",
    "# accuracy on the validation set.                                            #\n",
    "##############################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "# ...\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "##############################################################################\n",
    "#                             END OF YOUR CODE                               #\n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ATQ36gIi6q3"
   },
   "source": [
    "# Debug the training\n",
    "With the default parameters we provided above, you should get a validation accuracy of more than 0.40 on the validation set. This isn't very good.\n",
    "\n",
    "One strategy for getting insight into what's wrong is to plot the loss function and the accuracies on the training and validation sets during optimization.\n",
    "\n",
    "Another strategy is to visualize the weights that were learned in the first layer of the network. In most neural networks trained on visual data, the first layer weights typically show some visible structure when visualized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 730
    },
    "executionInfo": {
     "elapsed": 965,
     "status": "ok",
     "timestamp": 1628606570346,
     "user": {
      "displayName": "MANTAS BIRŠKUS",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh4p74VzcLbLKZFYUnmVzxKmHhFZC9ouHcczEsEmQ=s64",
      "userId": "00995227095641424292"
     },
     "user_tz": -180
    },
    "id": "vzntsKPii6q4",
    "outputId": "73907935-4045-472f-a331-6b34e6865194"
   },
   "outputs": [],
   "source": [
    "# Run this cell to visualize training loss and train / val accuracy\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.title('Training loss')\n",
    "plt.plot(solver.loss_history, 'o')\n",
    "plt.xlabel('Iteration')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.title('Accuracy')\n",
    "plt.plot(solver.train_acc_history, '-o', label='train')\n",
    "plt.plot(solver.val_acc_history, '-o', label='val')\n",
    "plt.plot([0.5] * len(solver.val_acc_history), 'k--')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='lower right')\n",
    "plt.gcf().set_size_inches(15, 12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 466
    },
    "executionInfo": {
     "elapsed": 1231,
     "status": "ok",
     "timestamp": 1628606717482,
     "user": {
      "displayName": "MANTAS BIRŠKUS",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh4p74VzcLbLKZFYUnmVzxKmHhFZC9ouHcczEsEmQ=s64",
      "userId": "00995227095641424292"
     },
     "user_tz": -180
    },
    "id": "rkEy0Cuui6q4",
    "outputId": "be25c33a-3b04-4f1a-dc6c-08dd8eb0495d"
   },
   "outputs": [],
   "source": [
    "from sjk012.vis_utils import visualize_grid\n",
    "\n",
    "# Visualize the weights of the network\n",
    "\n",
    "def show_net_weights(net):\n",
    "    W1 = net.params['W1']\n",
    "    W1 = W1.reshape(32, 32, 3, -1).transpose(3, 0, 1, 2)\n",
    "    plt.imshow(visualize_grid(W1, padding=3).astype('uint8'))\n",
    "    plt.gca().axis('off')\n",
    "    plt.show()\n",
    "\n",
    "show_net_weights(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HfPfAQSAi6q4"
   },
   "source": [
    "# Hyperparameter Tuning with Multi-threading\n",
    "\n",
    "To expedite the hyperparameter search process, consider leveraging multiple threads to train your model. \n",
    "\n",
    "As indicated by the linear decrease in loss and lack of gap between training and validation accuracy, adjustments to hyperparameters such as learning rate, model size, batch size, and regularization strength may be necessary.\n",
    "\n",
    "**Experimentation and Tuning**: Look a the hyperparameters including hidden layer size, learning rate, number of training epochs, batch size, and regularization strength. You may also explore tuning the learning rate decay, although satisfactory performance can often be achieved with the default value.\n",
    "\n",
    "**Target Performance**: Aim for a classification accuracy exceeding 48% on the validation set, with the best networks achieving over 52%.\n",
    "\n",
    "**Goal**: Your objective is to achieve optimal performance on CIFAR-10 with a fully-connected Neural Network. Use experimentation and hyperparameter tuning to iteratively refine your model's performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1199273,
     "status": "ok",
     "timestamp": 1628609888726,
     "user": {
      "displayName": "MANTAS BIRŠKUS",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh4p74VzcLbLKZFYUnmVzxKmHhFZC9ouHcczEsEmQ=s64",
      "userId": "00995227095641424292"
     },
     "user_tz": -180
    },
    "id": "4DhZpBUJi6q5",
    "outputId": "fc9ef387-bd58-48a6-f59c-5b5bc87da990"
   },
   "outputs": [],
   "source": [
    "best_model = None\n",
    "\n",
    "\n",
    "results = {}\n",
    "best_val = -1\n",
    "\n",
    "learning_rates = np.geomspace(3e-4, 3e-2, 3)\n",
    "regularization_strengths = np.geomspace(1e-6, 1e-2, 5)\n",
    "\n",
    "import itertools\n",
    "\n",
    "#################################################################################\n",
    "# TODO: Parallelise the hyperparmeter search procedure                          #\n",
    "#################################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "# ...\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################\n",
    "    \n",
    "    for lr, reg in itertools.product(learning_rates, regularization_strengths):\n",
    "        # Create Two Layer Net and train it with Solver\n",
    "        model = TwoLayerNet(hidden_dim=128, reg=reg)\n",
    "        solver = Solver(model, data, optim_config={'learning_rate': lr}, num_epochs=10, verbose=False)\n",
    "        solver.train()\n",
    "    \n",
    "        # Compute validation set accuracy and append to the dictionary\n",
    "        results[(lr, reg)] = solver.best_val_acc\n",
    "\n",
    "        # Save if validation accuracy is the best\n",
    "        if results[(lr, reg)] > best_val:\n",
    "            best_val = results[(lr, reg)]\n",
    "            best_model = model\n",
    "\n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "    val_accuracy = results[(lr, reg)]\n",
    "    print('lr %e reg %e val accuracy: %f' % (lr, reg, val_accuracy))\n",
    "    \n",
    "print('best validation accuracy achieved during cross-validation: %f' % best_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5pnOY1zOi6q5"
   },
   "source": [
    "# Test your model!\n",
    "Run your best model on the validation and test sets. You should achieve above 48% accuracy on the validation set and the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 299,
     "status": "ok",
     "timestamp": 1628609939257,
     "user": {
      "displayName": "MANTAS BIRŠKUS",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh4p74VzcLbLKZFYUnmVzxKmHhFZC9ouHcczEsEmQ=s64",
      "userId": "00995227095641424292"
     },
     "user_tz": -180
    },
    "id": "val_accuracy",
    "outputId": "13b3b89b-30cc-4e75-96fc-260183ad9c73"
   },
   "outputs": [],
   "source": [
    "y_val_pred = np.argmax(best_model.loss(data['X_val']), axis=1)\n",
    "print('Validation set accuracy: ', (y_val_pred == data['y_val']).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 220,
     "status": "ok",
     "timestamp": 1628609943973,
     "user": {
      "displayName": "MANTAS BIRŠKUS",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh4p74VzcLbLKZFYUnmVzxKmHhFZC9ouHcczEsEmQ=s64",
      "userId": "00995227095641424292"
     },
     "user_tz": -180
    },
    "id": "test_accuracy",
    "outputId": "3b4fdac1-f7fa-4062-db27-d2f5d2775f72"
   },
   "outputs": [],
   "source": [
    "y_test_pred = np.argmax(best_model.loss(data['X_test']), axis=1)\n",
    "print('Test set accuracy: ', (y_test_pred == data['y_test']).mean())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "two_layer_net.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
